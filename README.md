# Visual_Language_Tracking_Paper_List

## Papers

### AAAI 2024

- **UVLTrack:** <br>
  "Unifying Visual and Vision-Language Tracking via Contrastive Learning" AAAI 2024<br>
  [[paper](https://arxiv.org/pdf/2401.11228.pdf)] <br>
  [[code](https://github.com/OpenSpaceAI/UVLTrack)]


### CVPR 2023

- **JointNLT:** Li Zhou, Zikun Zhou, Kaige Mao, Zhenyu He<br>
  "Joint Visual Grounding and Tracking with Natural Language Specification" CVPR 2023 <br>
  [[paper](https://arxiv.org/pdf/2303.12027.pdf)]<br>
  [[code](https://github.com/lizhou-cs/JointNLT)]


### ICCV 2023

- **DecoupleTNL:** Ding Ma, Xiangqian Wu<br>
  "Tracking by Natural Language Specification with Long Short-term Context Decoupling" ICCV 2023<br>
  [[paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Tracking_by_Natural_Language_Specification_with_Long_Short-term_Context_Decoupling_ICCV_2023_paper.pdf)]


### NeurIPS 2023

- **MGIT:** Shiyu Hu, Dailin Zhang, Meiqi Wu, Xiaokun Feng, Xuchen Li, Xin Zhao, Kaiqi Huang<br>
  "A Multi-modal Global Instance Tracking Benchmark (MGIT): Better Locating Target in Complex Spatio-temporal and causal Relationship" NeurIPS 2023<br>
  [[paper](https://huuuuusy.github.io/files/MGIT.pdf)]<br>
  [[platform](http://videocube.aitestunion.com/)]


### ACM MM 2023

- **All in One:** Chunhui Zhang, Xin Sun, Li Liu, Yiqian Yang, Qiong Liu, Xi Zhou, Yanfeng Wang <br>
  "All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment"  ACM MM 2023<br>
  [[paper](https://arxiv.org/pdf/2307.03373.pdf)] <br>
  [[code](https://github.com/983632847/All-in-One)]


### TMM 2023 

- **OVLM:** Huanlong Zhang, Jingchao Wang, Jianwei Zhang, Tianzhu Zhang, Bineng Zhong <br>
  "One-stream Vision-Language Memory Network for Object Tracking" TMM 2023<br>
   [[paper](https://ieeexplore.ieee.org/abstract/document/10149530)]


### TCSVT 2023

- **MMTrack:** Yaozong Zheng, Bineng Zhong, Qihua Liang, Guorong Li, Rongrong Ji, Xianxian Li<br>
  "Towards Unified Token Learning for Vision-Language Tracking" TCSVT 2023<br>
  [[paper](https://ieeexplore.ieee.org/abstract/document/10208210)] <br>
  [[code](https://github.com/Azong-HQU/MMTrack)]

- **TransNLT:** Rong Wang, Zongheng Tang, Qianli Zhou, Xiaoqian Liu, Tianrui Hui, Quange Tan, Si Liu<br>
  "Unified Transformer With Isomorphic Branches for Natural Language Tracking" TCSVT 2023<br>
  [[paper](https://ieeexplore.ieee.org/document/10159158/)]


### PRL 2023 

- **TransVLT:** Haojie Zhao, Xiao Wang, Dong Wang, Huchuan Lu, Xiang Ruan<br>
  "Transformer vision-language tracking via proxy token guided cross-modal fusion"<br>
  [[paper](https://www.sciencedirect.com/science/article/pii/S0167865523000545?via%3Dihub)]

- **SATracker:** Jiawei Ge, Xiangmei Chen, Jiuxin Cao, Xuelin Zhu, Weijia Liu, Bo Liu<br>
  "Beyond Visual Cues: Synchronously Exploring Target-Centric Semantics for Vision-Language Tracking" arXiv 2023<br>
  [[paper](https://arxiv.org/pdf/2311.17085.pdf)]


### arXiv 2023
- **VLT_OST:** Mingzhe Guo, Zhipeng Zhang, Liping Jing , Haibin Ling, Heng Fan<br>
  "Divert More Attention to Vision-Language Object Tracking" arXiv 2023<br>
  [[paper](https://arxiv.org/pdf/2307.10046.pdf)]<br>
  [[code](https://github.com/JudasDie/SOTS)]


### NeuIPS 2022 

- **VLT_TT:** Mingzhe Guo, Zhipeng Zhang, Heng Fan, Liping Jing<br>
  "Divert More Attention to Vision-Language Tracking" NeurIPS 2022<br>
  [[paper](https://arxiv.org/pdf/2207.01076.pdf)] <br>
  [[code](https://github.com/JudasDie/SOTS)]


### CVPRW 2022
- **CTRTNL:** Yihao L, Jun Yu, Zhongpeng Cai, Yuwen Pan<br>
  "Cross-Modal Target Retrieval for Tracking by Natural Language" CVPRW 2022<br>
  [[paper](https://openaccess.thecvf.com/content/CVPR2022W/ODRUM/papers/Li_Cross-Modal_Target_Retrieval_for_Tracking_by_Natural_Language_CVPRW_2022_paper.pdf)]


### CVPR 2021

- **SNLT:** Qi Feng, Vitaly Ablavsky, Qinxun Bai, Stan Sclaroff<br>
  "Siamese Natural Language Tracker: Tracking by Natural Language Descriptions With Siamese Trackers" CVPR 2021<br>
  [[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Feng_Siamese_Natural_Language_Tracker_Tracking_by_Natural_Language_Descriptions_With_CVPR_2021_paper.pdf)]<br>
  [[code](https://github.com/fredfung007/snlt)]
  
- **TNL2K:** Xiao Wang, Xiujun Shu, Zhipeng Zhang, Bo Jiang, Yaowei Wang, Yonghong Tian, Feng Wu<br>
  "Towards More Flexible and Accurate Object Tracking with Natural Language: Algorithms and Benchmark" CVPR 2021<br>
  [[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Towards_More_Flexible_and_Accurate_Object_Tracking_With_Natural_Language_CVPR_2021_paper.pdf)]<br>
  [[platform](https://sites.google.com/view/langtrackbenchmark/)]


### ACM MM 2021

- **CapsuleTNL:** Ding Ma, Xiangqian Wu<br>
  "Capsule-based Object Tracking with Natural Language Specification" ACM MM 2021<br>
  [[paper](https://dl.acm.org/doi/abs/10.1145/3474085.3475349)] 


### TCSVT 2021

- **GTI:** Zhengyuan Yang, Tushar Kumar, Tianlang Chen, Jingsong Su, Jiebo Luo<br>
  "Grounding-Tracking-Integration" TCSVT 2021<br>
  [[paper](https://ieeexplore.ieee.org/abstract/document/9261416/)]


### WACV 2020
- **RTTNLD:** Qi Feng, Vitaly Ablavsky, Qinxun Bai, Guorong Li, Stan Sclaroff<br>
  "Real-time Visual Object Tracking with Natural Language Description" WACV 2020<br>
  [[paper](https://arxiv.org/pdf/1907.11751.pdf)]


### arXiv 2019
- **NLRPN:** Qi Feng, Vitaly Ablavsky, Qinxun Bai, Stan Sclaroff<br>
  "Robust Visual Object Tracking with Natural Language Region Proposal Network" arXiv 2019<br>
  [[paper](https://arxiv.org/pdf/1912.02048v1.pdf)]


### arXiv 2018
- **DAT:** Xiao Wang, Chenglong Li, Rui Yang, Tianzhu Zhang, Jin Tang, Bin Luo<br>
  "Describe and Attend to Track: Learning Natural Language guided Structural Representation and Visual Attention for Object Tracking" arXiv 2018<br>
  [[paper](https://arxiv.org/pdf/1811.10014.pdf)]


### CVPR 2017

- **TNLS:** Li, Zhenyang and Tao, Ran and Gavves, Efstratios, Snoek, Cees G. M., Smeulders, Arnold W. M.<br>
  "Tracking by Natural Language Specification" CVPR 2017<br>
  [[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Tracking_by_Natural_CVPR_2017_paper.pdf)] <br>
  [[code](https://github.com/zhenyangli/lang-tracker)]
